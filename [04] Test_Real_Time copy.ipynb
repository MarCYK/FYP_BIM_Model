{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from matplotlib import pyplot as plt \n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "# Check if webcam functionality is working\n",
    "import cv2\n",
    "\n",
    "cam = cv2.VideoCapture(1)\n",
    "\n",
    "while True:\n",
    "    check, frame = cam.read()\n",
    "\n",
    "    cv2.imshow('video', frame)\n",
    "\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\n",
    "import torch; print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mediapipie Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities\n",
    "\n",
    "def mediapipe_detection(image,model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Color conversion from BGR to RGB\n",
    "    image.flags.writeable = False                   # Image is no longer writeable\n",
    "    results = model.process(image)                  # Make prediction\n",
    "    image.flags.writeable = True                    # Image is no longer writeable\n",
    "    image = cv2.cvtColor(image,cv2.COLOR_RGB2BGR)   # Color conversion RGB to BGR\n",
    "    return image, results\n",
    "\n",
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)  # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)    # Draw right connections\n",
    "\n",
    "def draw_styled_landmarks(image,results):\n",
    "    # Draw pose connection\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(0,0,255), thickness=5,circle_radius=5),\n",
    "                              mp_drawing.DrawingSpec(color=(80,110,10), thickness=5,circle_radius=5)\n",
    "                              )\n",
    "    # Draw left hand connection\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(255, 255, 0), thickness=5,circle_radius=5),\n",
    "                              mp_drawing.DrawingSpec(color=(255, 255, 0), thickness=5,circle_radius=5)\n",
    "                              )\n",
    "    # Draw right hand connection\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(255, 255, 0), thickness=5,circle_radius=5),\n",
    "                              mp_drawing.DrawingSpec(color=(255, 255, 0), thickness=5,circle_radius=5)\n",
    "                              )\n",
    "    \n",
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose,lh,rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [(245,117,16), (117,245,16), (16,117,245)]\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gestures:  107 ; Total videos:  7098\n"
     ]
    }
   ],
   "source": [
    "video_directory = 'TRAIN_5'\n",
    "\n",
    "sum = 0\n",
    "\n",
    "gesture_folder = np.array(os.listdir(video_directory))\n",
    "for gestures in gesture_folder:\n",
    "    gesture = []\n",
    "\n",
    "    for fname in os.listdir(os.path.join(video_directory, gestures)):\n",
    "        path = os.path.join(video_directory, gestures, fname)\n",
    "        if os.path.isdir(path):\n",
    "            gesture.append(fname)\n",
    "\n",
    "    sum += len(gesture) \n",
    "    # print(gestures, end =\" : \")        \n",
    "    # print(len(gesture))\n",
    "\n",
    "print(\"Total gestures: \", len(gesture_folder), \"; Total videos: \", sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['abang', 'ada', 'adik_lelaki', 'adik_perempuan', 'air', 'ambil',\n",
       "       'anak', 'apa', 'arah', 'awak', 'ayah', 'baca', 'bagaimana', 'baik',\n",
       "       'baik2', 'bas', 'bawa', 'belajar', 'beli', 'beli2', 'berapa',\n",
       "       'berjalan', 'berlari', 'bila', 'bola', 'boleh', 'bomba', 'buang',\n",
       "       'buat', 'cuaca', 'curi', 'dapat', 'dari', 'datuk', 'duit', 'esok',\n",
       "       'gambar', 'hari', 'hilang', 'hospital', 'hujan', 'ibu', 'jahat',\n",
       "       'jalan', 'jam', 'jangan', 'jumpa', 'kacau', 'kafeteria', 'kakak',\n",
       "       'kedai', 'keluarga', 'kereta', 'kereta_api', 'khabar_baik',\n",
       "       'lelaki', 'lupa', 'main', 'makan', 'mana', 'marah', 'marah2',\n",
       "       'mari', 'mari2', 'masa', 'masalah', 'menyakitkan', 'minum',\n",
       "       'mohon', 'mohon2', 'nama', 'nasi', 'nasi_lemak', 'nenek', 'panas',\n",
       "       'panas2', 'pandai', 'pandai2', 'payung', 'pen', 'pensil',\n",
       "       'perempuan', 'pergi', 'pergi2', 'perlahan', 'perlahan2', 'polis',\n",
       "       'pukul', 'ribut', 'sampai', 'saudara', 'saya', 'sejuk', 'sekolah',\n",
       "       'selamat_pagi', 'siapa', 'sudah', 'suka', 'tandas', 'tanya',\n",
       "       'teh_tarik', 'teksi', 'terima_kasih', 'tidur', 'tolong', 'tolong2',\n",
       "       'waalaikumssalam'], dtype='<U15')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gestures = np.array(gesture_folder)\n",
    "gestures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map = {label: num for num, label in enumerate(gesture_folder)}\n",
    "len(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MCYK\\AppData\\Local\\Temp\\ipykernel_4156\\320542830.py:36: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_model_state_dict = torch.load(model_filename)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CustomLSTM(\n",
       "  (lstm1): LSTM(258, 64, batch_first=True)\n",
       "  (lstm2): LSTM(64, 64, batch_first=True)\n",
       "  (lstm3): LSTM(64, 64, batch_first=True)\n",
       "  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc4): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (fc5): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (output_layer): Linear(in_features=32, out_features=107, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%script false\n",
    "# Define your custom LSTM model\n",
    "class CustomLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(CustomLSTM, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.fc5 = nn.Linear(32, 32)\n",
    "        self.output_layer = nn.Linear(32, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x, _ = self.lstm3(x)\n",
    "        x = torch.relu(self.fc1(x[:, -1, :]))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = torch.relu(self.fc5(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "# Instantiate the model\n",
    "input_size = 258\n",
    "hidden_size = 64\n",
    "num_classes = len(label_map)\n",
    "model = CustomLSTM(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Load the saved model state dictionary\n",
    "model_filename = 'models/lstm_model_train_5_0.88.pth'\n",
    "loaded_model_state_dict = torch.load(model_filename)\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "model.load_state_dict(loaded_model_state_dict)\n",
    "model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false\n",
    "# load transformer model\n",
    "# Define Transformer model for classification\n",
    "class CustomTransformer(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, d_model=64, nhead=8, num_encoder_layers=3, dim_feedforward=128, dropout=0.1):\n",
    "        super(CustomTransformer, self).__init__()\n",
    "        self.input_projection = nn.Linear(input_size, d_model)  # Project input to model dimension\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, 5000, d_model))  # Positional Encoding\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=d_model,\n",
    "                nhead=nhead,\n",
    "                dim_feedforward=dim_feedforward,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,  # Ensure batch is first dim\n",
    "            ),\n",
    "            num_layers=num_encoder_layers,\n",
    "        )\n",
    "        self.fc = nn.Linear(d_model, num_classes)  # Final classification layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Project input to d_model dimension\n",
    "        x = self.input_projection(x)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.positional_encoding[:, :seq_len, :]\n",
    "        \n",
    "        # Pass through Transformer Encoder\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        # Take the last token's representation for classification\n",
    "        x = x[:, -1, :]\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "# Model, loss, and optimizer\n",
    "input_size = 258\n",
    "num_classes = len(label_map)\n",
    "model = CustomTransformer(input_size=input_size, num_classes=num_classes)\n",
    "\n",
    "# Load the saved model state dictionary\n",
    "model_filename = 'models/transformer_model_train_2_0.96.pth'\n",
    "loaded_model_state_dict = torch.load(model_filename)\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "model.load_state_dict(loaded_model_state_dict)\n",
    "model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false\n",
    "# 1. New detection variables\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.3\n",
    "frame_count = 0\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "        \n",
    "        if len(sequence) == 30:\n",
    "            res = model(torch.tensor(np.expand_dims(sequence, axis=0),dtype=torch.float32))\n",
    "            print(gestures[res.argmax(dim=1)])\n",
    "            predictions.append(res.argmax(dim=1))\n",
    "\n",
    "            # prediction logic\n",
    "            if np.unique(predictions[-10:])[0]==res.argmax(dim=1): \n",
    "                if res.argmax(dim=1) > threshold: \n",
    "                    \n",
    "                    if len(sentence) > 0: \n",
    "                        if gestures[res.argmax(dim=1)] != sentence[-1]:\n",
    "                            sentence.append(gestures[res.argmax(dim=1)])\n",
    "                    else:\n",
    "                        sentence.append(gestures[res.argmax(dim=1)])\n",
    "\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "        if frame_count >= 30:\n",
    "            frame_count = 0\n",
    "\n",
    "        # Display frame count\n",
    "        cv2.putText(image, f'Frame: {frame_count}', (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "            \n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (1,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "\n",
    "# Visualization function\n",
    "def plot_sequence(sequence):\n",
    "    seq_array = np.array(sequence)\n",
    "    fig, ax = plt.subplots(figsize=(4, 2))\n",
    "    ax.imshow(seq_array.T, aspect='auto', interpolation='nearest', cmap='viridis')\n",
    "    ax.set_title(\"Sequence Heatmap\")\n",
    "    ax.set_xlabel(\"Frame\")\n",
    "    ax.set_ylabel(\"Keypoints\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    canvas = FigureCanvas(fig)\n",
    "    canvas.draw()\n",
    "    img = np.frombuffer(canvas.tostring_rgb(), dtype=np.uint8)\n",
    "    img = img.reshape(canvas.get_width_height()[::-1] + (3,))\n",
    "    plt.close(fig)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MCYK\\AppData\\Local\\Temp\\ipykernel_16904\\3577891941.py:15: MatplotlibDeprecationWarning: The tostring_rgb function was deprecated in Matplotlib 3.8 and will be removed in 3.10. Use buffer_rgba instead.\n",
      "  img = np.frombuffer(canvas.tostring_rgb(), dtype=np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nasi_lemak\n",
      "nasi_lemak\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MCYK\\.conda\\envs\\BIM_gpu\\lib\\site-packages\\numpy\\lib\\arraysetops.py:272: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  ar = np.asanyarray(ar)\n",
      "c:\\Users\\MCYK\\.conda\\envs\\BIM_gpu\\lib\\site-packages\\numpy\\lib\\arraysetops.py:272: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ar = np.asanyarray(ar)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "curi\n",
      "anak\n",
      "anak\n",
      "curi\n",
      "anak\n",
      "curi\n",
      "curi\n",
      "curi\n",
      "curi\n",
      "curi\n",
      "anak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "hari\n",
      "hari\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "hari\n",
      "hari\n",
      "hari\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "nasi_lemak\n",
      "hari\n",
      "hari\n",
      "hari\n",
      "hari\n",
      "hari\n",
      "hari\n",
      "hari\n",
      "hari\n",
      "hari\n",
      "hari\n",
      "hari\n",
      "hari\n",
      "hari\n",
      "hari\n",
      "hari\n",
      "hari\n",
      "hari\n",
      "hari\n",
      "hari\n",
      "hari\n",
      "hari\n",
      "hari\n",
      "hari\n",
      "hari\n",
      "hari\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "hospital\n",
      "anak\n",
      "hospital\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "hospital\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n",
      "anak\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.3\n",
    "frame_count = 0\n",
    "\n",
    "mp_holistic = mp.solutions.holistic\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "        # Check if hand landmarks are visible\n",
    "        if results.left_hand_landmarks or results.right_hand_landmarks:\n",
    "            frame_count += 1\n",
    "            \n",
    "            # Draw landmarks\n",
    "            draw_styled_landmarks(image, results)\n",
    "            \n",
    "            # Prediction logic\n",
    "            keypoints = extract_keypoints(results)\n",
    "            sequence.append(keypoints)\n",
    "            sequence = sequence[-30:]\n",
    "            \n",
    "            if len(sequence) == 30:\n",
    "                res = model(torch.tensor(np.expand_dims(sequence, axis=0), dtype=torch.float32))\n",
    "                print(gestures[res.argmax(dim=1)])\n",
    "                predictions.append(res.argmax(dim=1))\n",
    "\n",
    "                # Prediction stability logic\n",
    "                if np.unique(predictions[-10:])[0] == res.argmax(dim=1): \n",
    "                    if res.max() > threshold: \n",
    "                        \n",
    "                        if len(sentence) > 0: \n",
    "                            if gestures[res.argmax(dim=1)] != sentence[-1]:\n",
    "                                sentence.append(gestures[res.argmax(dim=1)])\n",
    "                        else:\n",
    "                            sentence.append(gestures[res.argmax(dim=1)])\n",
    "\n",
    "                if len(sentence) > 5: \n",
    "                    sentence = sentence[-5:]\n",
    "\n",
    "            if frame_count >= 30:\n",
    "                frame_count = 0\n",
    "\n",
    "            # Display frame count\n",
    "            cv2.putText(image, f'Frame: {frame_count}', (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "            \n",
    "            # Heatmap visualization\n",
    "            if len(sequence) > 0:\n",
    "                heatmap_image = plot_sequence(sequence[-30:])  # Get the last 30 frames\n",
    "                heatmap_image = cv2.cvtColor(heatmap_image, cv2.COLOR_RGB2BGR)  # Convert to BGR\n",
    "                h, w, _ = heatmap_image.shape\n",
    "                # Overlay the heatmap in the top-left corner\n",
    "                image[0:h, 0:w, :] = heatmap_image\n",
    "                \n",
    "            # Display predictions and sequence\n",
    "            cv2.rectangle(image, (0, 0), (640, 40), (245, 117, 16), -1)\n",
    "            cv2.putText(image, ' '.join(sentence), (10, 30), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        else:\n",
    "            # Display message if no hands are detected\n",
    "            cv2.putText(image, \"No hand detected\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BIM_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
